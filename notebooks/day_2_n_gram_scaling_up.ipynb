{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd532aad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Here's our N-gram model: what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66071bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"By this liberty they entered into a very laudable emulation to do all of them \\\n",
    "what they saw did please one. If any of the gallants or ladies should say, Let us drink, \\\n",
    "they would all drink.  If any one of them said, Let us play, they all played.  If one said, \\\n",
    "Let us go a-walking into the fields they went all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ngrams for a corpus\n",
    "def ngrams(text, n):\n",
    "    n_grams = []\n",
    "    for i in range(n-1, len(tokenized_corpus)): \n",
    "        n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "    return n_grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0792006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NgramModel():\n",
    "\n",
    "    def __init__(self, corpus, n,music=True):\n",
    "        self.n = n\n",
    "        if music:\n",
    "            tokenized_corpus = corpus\n",
    "        if not music:\n",
    "            tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "        self.music = music\n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        \n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        # separate punctuation from previous word\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        \n",
    "        # split into sentences\n",
    "        sentences = spaced_corpus.split('.')\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokenized_corpus += words\n",
    "        \n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "\n",
    "    def generate_music(self, length = 10):\n",
    "        seed = random.choice(tokenized_corpus)\n",
    "    \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "        \n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be51",
   "metadata": {},
   "source": [
    "# Scaling up\n",
    "\n",
    "If we keep increasing n, our generated text starts to repeat our input text almost word for word. To get interesting behavior, we have to increase the size of the corpus. Let's try with a much bigger corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6702e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!python3 -m nltk.downloader gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f955ed1",
   "metadata": {},
   "source": [
    "NLTK comes with several built in corpora, including a selection of books from project gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c38e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpus using an alias to avoid namespace confustion with our corpus variable\n",
    "from nltk import corpus as corpiss \n",
    "\n",
    "corpiss.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e69afbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Leaves',\n",
       " 'of',\n",
       " 'Grass',\n",
       " 'by',\n",
       " 'Walt',\n",
       " 'Whitman',\n",
       " '1855',\n",
       " ']',\n",
       " 'Come',\n",
       " ',',\n",
       " 'said',\n",
       " 'my',\n",
       " 'soul',\n",
       " ',',\n",
       " 'Such',\n",
       " 'verses',\n",
       " 'for',\n",
       " 'my',\n",
       " 'Body',\n",
       " 'let',\n",
       " 'us',\n",
       " 'write',\n",
       " ',',\n",
       " '(',\n",
       " 'for',\n",
       " 'we',\n",
       " 'are',\n",
       " 'one',\n",
       " ',)',\n",
       " 'That',\n",
       " 'should',\n",
       " 'I',\n",
       " 'after',\n",
       " 'return',\n",
       " ',',\n",
       " 'Or',\n",
       " ',',\n",
       " 'long',\n",
       " ',']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv = corpiss.gutenberg.words('whitman-leaves.txt')\n",
    "kjv2 = corpiss.gutenberg.words('bible-kjv.txt')\n",
    "list(kjv)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cfdb2",
   "metadata": {},
   "source": [
    "### A Few final adjustments\n",
    "\n",
    "We have some housekeeping things to take care of. \n",
    "\n",
    "1. Because we have encoded sentence breaks as a string of start and stop sequences, we now will generate a lot of them in our output. We add function to strip them out, and update our generate method to strip out these tokens before printing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "703885bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son . even so , father ; for the press is full , the fats overflow ; for their wickedness . 119 : 59 i thought on my ways , as a seal upon him , and would none of my words . 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz . 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab . 134 : 3 the aged women likewise , that they escaped all safe to land . spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up . unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning . 106 : 18 and samuel told him every whit , and hid snares for my feet .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_stops(string):\n",
    "    \"\"\"\n",
    "    function to convert the stop/start sequence back into periods.\n",
    "    strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "    and replaces them with a period.\n",
    "\n",
    "    then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "    string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "    string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "test_string = '<s> <s> <s> take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son </s> </s> </s> <s> <s> <s> even so , father ; for the press is full , the fats overflow ; for their wickedness </s> </s> </s> <s> <s> <s> 119 : 59 i thought on my ways , as a seal upon him , and would none of my words </s> </s> </s> <s> <s> <s> 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz </s> </s> </s> <s> <s> <s> 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab </s> </s> </s> <s> <s> <s> 134 : 3 the aged women likewise , that they escaped all safe to land </s> </s> </s> <s> <s> <s> spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up </s> </s> </s> <s> <s> <s> unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning </s> </s> </s> <s> <s> <s> 106 : 18 and samuel told him every whit , and hid snares for my feet </s> </s> </s> <s> <s> <s>'\n",
    "model = NgramModel(corpus, 3)\n",
    "add_stops(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b86ba",
   "metadata": {},
   "source": [
    "2. numbers are a problem for n-gram models becayse there are so many of them. we don't want to eliminate them, because they are meaningful, but we want to abstract away from the individual numbers. In addition, we might want to get rid of some other things like parentheticals and quotes, becayse these impossible for our model to keep track of given it's amount of memory. We can take care of these things in the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f57c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def _tokenize(self, corpus):\n",
    "    # The list of regular expressions and replacements to be applied\n",
    "    # the order here matters! these replacements will happen in order\n",
    "    replacements = [\n",
    "         [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "        ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "        ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "        ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "        ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "    ]\n",
    "\n",
    "    # This is a function that applies a single replacement from the list\n",
    "    resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "\n",
    "    # we use the resub function to applea each replacement to the entire corpus,\n",
    "    normalized_corpus = reduce(resub, replacements, corpus)\n",
    "\n",
    "\n",
    "    sentences = normalized_corpus.split('.')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split() # split on whitespace\n",
    "        words = [word.lower() for word in words]\n",
    "        words = list(pad_both_ends(words, n=self.n))\n",
    "        tokens += words\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b11df",
   "metadata": {},
   "source": [
    "Here is a final version of our class with all the bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e46957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from functools import reduce\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "\n",
    "    def _tokenize(self, corpus):\n",
    "        # The list of regular expressions and replacements to be applied\n",
    "        # the order here matters! these replacements will happen in order\n",
    "        replacements = [\n",
    "             [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "            ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "            ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "            ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "            ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "        ]\n",
    "        \n",
    "        # This is a function that applies a single replacement from the list\n",
    "        resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "        \n",
    "        # we use the resub function to applea each replacement to the entire corpus,\n",
    "        normalized_corpus = reduce(resub, replacements, corpus)\n",
    "        \n",
    "        \n",
    "        sentences = normalized_corpus.split('.')\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokens += words\n",
    "        \n",
    "        return tokens\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            \n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            if next_token == '</s>':\n",
    "                continue\n",
    "            \n",
    "            # get the last n-1 tokens to condition on next\n",
    "            lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "            next_token = self.cpd[lessgram].generate()\n",
    "            string.append( next_token )\n",
    "\n",
    "        string = ''.join(string)\n",
    "        string = add_stops(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def generate_chars(self, char_limit = 100, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "\n",
    "        while len(''.join(string)) < char_limit:\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ''.join(string)[:char_limit]\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    def add_stops(string):\n",
    "        \"\"\"\n",
    "        function to convert the stop/start sequence back into periods.\n",
    "        strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "        and replaces them with a period.\n",
    "\n",
    "        then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "        string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "        string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a84c5b77-e50b-42da-8a55-f73a1af12171",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACDC_solo = [-4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0, -4, 3, -4, 0, -2, 0, -4, 0]\n",
    "import random\n",
    "\n",
    "def ngrams(l, n):\n",
    "    return [tuple(l[i:i+n]) for i in range(len(text) - n + 1)]\n",
    "\n",
    "def ngram_frequency(ngram_list):\n",
    "    return {x: ngram_list.count(x) for x in ngram_list}\n",
    "\n",
    "def counts(frequencies):\n",
    "    d = {x[0] : {} for x in frequencies.keys()}\n",
    "    for x in frequencies.keys():\n",
    "        d[x[0]][x[1]] = frequencies[x]\n",
    "    retrun d\n",
    "\n",
    "def generate_solo(seed_solo, length):\n",
    "    \n",
    "    all_2grams = ngrams(seed_solo, 2)\n",
    "    freqs = ngram_frequency(all_2grams)\n",
    "    c = counts(freqs)\n",
    "\n",
    "    \n",
    "    next_note = random.choice(seed_solo)\n",
    "\n",
    "    solo = [next_note]\n",
    "    \n",
    "    for i in range(length-1):\n",
    "        options = c[next_note].keys()\n",
    "        tot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "966c2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(' '.join(kjv2) + ' '.join(kjv), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6c62143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it was ; me he restored unto mine altar ; it containeth much . [number] year of nebuchadrezzar . unfolded out of his throne , were taken captives , saying to jerusalem unto this place ? and wherefore slew he him up for me a long , beseems to day from abiding in the thirteenth day of midian and ephah , caleb the son of geber , in life by one great pitchy torch stationary with wild red and green , again in florida i float thee a desolation unto this treasurer , and say before him whom ye call the perfection thereof upon it shall lie all night , thy barns all fill ' d sideways or backward towards me to minister by reason of strength , haste , i dispose of nothing else , go \""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591a9ce",
   "metadata": {},
   "source": [
    "We try generating a 4-gram model with the King James Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20358",
   "metadata": {},
   "source": [
    "Our model expects its training corpus in the form of a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ee616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = (' ').join(kjv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be7089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(kjv, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2637c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for this purpose , to make himself an everlasting name ? [number] : [number] noah , shem , ham , and the prophet gad , david ' s uncle shall take him into the castle . furthermore i tell thee , and every high thing that exalteth itself against the knowledge of thy ways ? [number] : [number] fret not thyself because of evil men , neither be ye grieved . hear now , hananiah ; [number] : [number] and went to rabbah , and it bred worms , and stank : and moses was content to dwell with him among the merchants ? [number] : [number] wilt thou hunt the prey for the lion ? or fill the appetite of the young lions roar after their prey , and that sware unto me , fear not , thou hast clothing , be thou removed , and placed in the chariot cities , and from mizpeh of gilead , and over medeba : on all their heads . amos [number] : [number] envyings , murders , [number] : [number] sibbecai the hushathite , [number] : [number] for which i boast of you to them of whom i hear such things ? for i am ready not to be done in earth , or things to come ; because thou wast a god that hath caused his name to zedekiah \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1d6e2",
   "metadata": {},
   "source": [
    "# Let's do a mashup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c1a90",
   "metadata": {},
   "source": [
    "Intro to beautiful soup for scraping web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86054ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "#cover t-i-the-invisible-committe-now-8.png\n",
      "#pubdate 2018-02-25 19:37:42 +0000\n",
      "#title Now\n",
      "#author The Invisible Committe\n",
      "#LISTtitle Now\n",
      "#SORTauthors comité invisible\n",
      "#date 2017\n",
      "#source Retrived on February 18, 2018 from https://illwilleditions.noblogs.org/files/2018/02/Invisible-Committee-NOW-READ.pdf\n",
      "#lang en\n",
      "#SORTtopics insurrectionary, communization\n",
      "#notes The Invisible Committee are an anonymous fragment of the Imaginary Party.\n",
      "First published as *Maintenant* in May, 2017.\n",
      "Translated by Robert Hurley.\n",
      "\n",
      "\n",
      "No more waiting.\n",
      "No more hoping.\n",
      "No more letting ourselves be distracted, unnerved.\n",
      "Break and enter.\n",
      "Put untruth back in its place.\n",
      "Believe in what we feel.\n",
      "Act accordingly.\n",
      "Force our way into the present.\n",
      "Try. Fail this time. Try again. Fail better.\n",
      "Persist. Attack. Build.\n",
      "Go down one’s road.\n",
      "Win perhaps.\n",
      "In any case, overcome.\n",
      "Live, therefore.\n",
      "Now...\n",
      "\n",
      "\n",
      "** Tomorrow Is Cancelled\n",
      "\n",
      "\n",
      "\n",
      "[[t-i-the-invisible-committe-now-1.png f]]\n",
      "\n",
      "\n",
      "\n",
      "All the reasons for making a revolution are there. Not one is lacking. The shipwreck of politics, the arrogance of the powerful, the reign of falsehood, the vulgarity of the wealthy, the cataclysms of industry, galloping misery, naked exploitation, ecological apocalypse—we are spared nothing, not even being informed about it all. “Climate: 2016 breaks a heat record,” Le Monde announces, the same as almost every year now. All the reasons are there together, but it’s not reasons that make revolutions, it’s bodies. And the bodies are in front of screens.\n",
      "\n",
      "One can watch a presidential election sink like a stone. The transformation of “the most important moment in French political life” into a big trashing fest only makes the soap opera more captivating. One couldn’t imagine Koh-Lanta with such characters, such dizzying plot twists, such cruel tests, or so general a humiliation. The spectacle of politics lives on as the spectacle of its decomposition. Disbelief goes nicely with the filthy landscape. The National Front, that political negation of\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/the-invisible-committe-now.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "print(text[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57d0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4495207\n",
      "199704\n",
      "3994080\n"
     ]
    }
   ],
   "source": [
    "print(len(kjv))\n",
    "print(len(text))\n",
    "print(len(text * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f269771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mashup = text * 20 + kjv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1277f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(mashup, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a024dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[number] : gad called night season . commodity exchange for their tongue faileth . [number] thus edom revolted . whose judgment proceedeth blessing that very much abroad as had privatized the production , yonder side shall endure sound that is there is to in the dead because it’s going forth for thine head uncovered , the children after brought joseph lived an institution idea is put together therefore king that the opposite , and our need not i will consider them named goliath , legalizing one ends that no children or “petty bourgeois”—and not doing any paradox , swelled the institutions , “republican ,” both fall one a basic asymmetry between the oases are everlasting gospel according to daniel prospered . [number] therefore serve me where christ in samaria instead with water and caught in the family '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363b0b1",
   "metadata": {},
   "source": [
    "Wow!\n",
    "\n",
    "'among the hills that are weaned from the waters saw thee polluted in thy glory above all people , from beersheba to mount up with . [number] : [number] and shaalabbin , and partly broken . report , that jehoshaphat the king is among us still believed in hope ; patient in spirit ; and half of thy power preserve thou those that served in the womb : if jacob take a lump of figs were set there upon him shall inherit all things thereon . in most militants this search for my gold and the lord separated the sons shall eat clean provender , which loveth thee and abishai , and kings have had dominion over our cattle . then he sacrificed also and to whomsoever he will prosper us ; thus have been occupied therein . yellowed figures of cherubims and palm trees : they serve not thy left side , upon their altars : but according to our hand be upon every fowl of the european union . all these did moses command joshua , this do ye look on us ; because a deep sleep fell upon it before saul : [number] wise men , let them turn their mourning . after theo’s rape , a strong wind ? [number] : [number] open thou mine affliction . to him remaining . rather comically , he took counsel how they might attain to innocency ? [number] : [number] for behold the place hormah  '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646db1d",
   "metadata": {},
   "source": [
    "# Exercise / Homework??\n",
    "\n",
    "Make a mashup of two texts. They can be texts you wrote (a collection of tweets, an essay), or from anywhere. You can use libgen to find books and Calibre to convert them to text. Either paste the text directly into a notebook or use a Python utility for reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15494939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150118\n",
      "199704\n"
     ]
    }
   ],
   "source": [
    "alice = (' ').join(corpiss.gutenberg.words('carroll-alice.txt'))\n",
    "now = text\n",
    "\n",
    "print(len(alice))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashups = alice + text\n",
    "model = NgramModel(mashups, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33bd1a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"along hand again in saying “me ! i couldn ' collar that we ourselves singularly , from an actual certificate of anything about here till his claws and essentially dispersive . comfort—which clouds perceptions , people had their turns out with ! thump ! oh dear dinah ! thump ! coming different resulting in industrial production team as “being an admirable way the hierarchy . yellowed figures of receptacle of pages would deny it scatters purposes and reaching half times over ; in prison ,' father ; before illustrates what am . with evaluation that can it back/would it up emerging as the looking anxiously . economy beyond all those combating the crevard , running when we work everywhere there for it’s as often push the money and said to stepping over by it ,' as usual leninists , a corps \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9cef5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "most used: \n",
    "* https://notebook.community/luketurner/ipython-notebooks/notebooks/n-gram%20tutorial\n",
    "* https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "* https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "others:\n",
    "* https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520\n",
    "* https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "* http://www.pygaze.org/2016/03/how-to-code-twitter-bot/\n",
    "    - code: https://github.com/esdalmaijer/markovbot\n",
    "* https://towardsdatascience.com/implementing-a-character-level-trigram-language-model-from-scratch-in-python-27ca0e1c3c3f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
